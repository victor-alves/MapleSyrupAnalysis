{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bitcapstoneconda4c011bf3812e465f9a235200dba099ea",
   "display_name": "Python 3.7.0 64-bit ('Capstone': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 vals per year\n",
    "# - missing 1: fill with median of that county at that year\n",
    "# - missing 2: fill with median of that county at that year\n",
    "# - missing 3: fill with median of that county at that year\n",
    "# median over mean:\n",
    "# - the median is less affected by outliers and skewed data than the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/raw/environment data - with counties.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"COUNTY\", \"ELEVATION\", \"DATE\", \"CDSD\", \"EMXP\", \"PRCP\", \"CLDD\", \"DT00\", \"DT32\", \"DX32\", \"DX70\", \"DX90\", \"EMNT\", \"EMXT\", \"FZF0\", \"FZF1\", \"FZF2\", \"FZF3\", \"FZF4\", \"FZF5\", \"FZF6\", \"FZF7\", \"FZF8\", \"FZF9\", \"HTDD\", \"TAVG\", \"TMAX\", \"TMIN\", \"SNOW\", \"PSUN\"]\n",
    "# consider whether emnt_attribute is needed for the date down the line\n",
    "target_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "relevant = df[relevant_cols].drop_duplicates()\n",
    "focused = df[target_cols]\n",
    "\n",
    "# categorize columns\n",
    "categorical_cols = list(relevant.select_dtypes(\"object\").columns)\n",
    "#id_cols = [col for col in training.columns if col.endswith(\"_id\")]\n",
    "id_cols = [\"ELEVATION\", \"DATE\"]\n",
    "target_col = []\n",
    "exclude_cols = categorical_cols + id_cols + target_col\n",
    "numerical_cols = [col for col in relevant.columns if col not in exclude_cols]\n",
    "\n",
    "# categorize columns for future use\n",
    "categorical_cols = list(relevant.select_dtypes(\"object\").columns)\n",
    "numerical_cols = [col for col in relevant.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = relevant.sort_values(by=[\"COUNTY\", \"DATE\"]).reset_index(drop=True).drop([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_groupby = relevant.groupby([\"COUNTY\", \"DATE\"])\n",
    "\n",
    "for county in relevant[\"COUNTY\"].unique():\n",
    "    county_data = relevant[relevant[\"COUNTY\"] == county]\n",
    "    for year in county_data[\"DATE\"].unique():\n",
    "        # get current slice of data from that year for that county\n",
    "        current_group = relevant_groupby.get_group((county, year))\n",
    "        # get the names of the columns that are missing all their data\n",
    "        missing_cols = current_group.loc[:, current_group.isna().sum() == current_group.shape[0]].columns\n",
    "        # get the names of the columns that have some missing data but not all\n",
    "        other_cols = [col for col in current_group.columns if col not in missing_cols]\n",
    "        # fill in the missing values in the columns in other_cols with their respective medians\n",
    "        current_group[other_cols] = current_group[other_cols].fillna(current_group[other_cols].median())\n",
    "        # save values in current group back into original dataframe ('relevant')\n",
    "        relevant.update(current_group, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATA INTO TWO - ONE FOR PREDICTIONS, ONE FOR CLUSTERING\n",
    "clustering = relevant.copy()\n",
    "predictions = relevant.copy()\n",
    "for col in predictions.columns:\n",
    "    missing_percentage = relevant[col].isna().sum() / predictions.shape[0] * 100\n",
    "    # set a threshhold to drop columns\n",
    "    if missing_percentage < 50.0:\n",
    "        predictions[col] = predictions[col].interpolate(method=\"linear\")\n",
    "    # if it exceeds it, drop the column?\n",
    "    else:\n",
    "        predictions = predictions.drop(columns=[col], axis=1)\n",
    "\n",
    "clustering = clustering.fillna(-9999999) # WIP\n",
    "predictions = predictions.fillna(predictions.median()).groupby([\"COUNTY\", \"DATE\"]).median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COUNTY</th>\n      <th>DATE</th>\n      <th>CDSD</th>\n      <th>EMXP</th>\n      <th>PRCP</th>\n      <th>CLDD</th>\n      <th>DT00</th>\n      <th>DT32</th>\n      <th>DX32</th>\n      <th>DX70</th>\n      <th>...</th>\n      <th>FZF5</th>\n      <th>FZF6</th>\n      <th>FZF7</th>\n      <th>FZF8</th>\n      <th>FZF9</th>\n      <th>HTDD</th>\n      <th>TAVG</th>\n      <th>TMAX</th>\n      <th>TMIN</th>\n      <th>SNOW</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Addison</td>\n      <td>1990</td>\n      <td>314.0</td>\n      <td>2.520</td>\n      <td>48.940</td>\n      <td>314.0</td>\n      <td>2.0</td>\n      <td>136.0</td>\n      <td>55.0</td>\n      <td>107.0</td>\n      <td>...</td>\n      <td>24.0</td>\n      <td>24.0</td>\n      <td>24.0</td>\n      <td>17.0</td>\n      <td>11.0</td>\n      <td>7198.0</td>\n      <td>47.00</td>\n      <td>54.20</td>\n      <td>38.50</td>\n      <td>69.40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Addison</td>\n      <td>1991</td>\n      <td>314.0</td>\n      <td>1.900</td>\n      <td>37.150</td>\n      <td>314.0</td>\n      <td>30.0</td>\n      <td>161.0</td>\n      <td>63.0</td>\n      <td>109.0</td>\n      <td>...</td>\n      <td>30.0</td>\n      <td>27.0</td>\n      <td>20.0</td>\n      <td>20.0</td>\n      <td>11.0</td>\n      <td>7941.5</td>\n      <td>42.90</td>\n      <td>53.60</td>\n      <td>32.30</td>\n      <td>57.70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Addison</td>\n      <td>1992</td>\n      <td>209.0</td>\n      <td>1.660</td>\n      <td>34.510</td>\n      <td>209.0</td>\n      <td>28.0</td>\n      <td>174.0</td>\n      <td>58.0</td>\n      <td>105.0</td>\n      <td>...</td>\n      <td>31.0</td>\n      <td>26.0</td>\n      <td>24.0</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>7909.5</td>\n      <td>43.90</td>\n      <td>53.40</td>\n      <td>31.40</td>\n      <td>22.50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Addison</td>\n      <td>1993</td>\n      <td>264.5</td>\n      <td>1.925</td>\n      <td>39.240</td>\n      <td>264.5</td>\n      <td>42.0</td>\n      <td>192.0</td>\n      <td>70.5</td>\n      <td>105.5</td>\n      <td>...</td>\n      <td>30.5</td>\n      <td>28.0</td>\n      <td>21.5</td>\n      <td>19.5</td>\n      <td>9.5</td>\n      <td>8435.5</td>\n      <td>41.25</td>\n      <td>53.10</td>\n      <td>29.40</td>\n      <td>129.95</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Addison</td>\n      <td>1994</td>\n      <td>325.5</td>\n      <td>1.520</td>\n      <td>40.745</td>\n      <td>325.5</td>\n      <td>42.0</td>\n      <td>180.5</td>\n      <td>69.0</td>\n      <td>102.0</td>\n      <td>...</td>\n      <td>30.5</td>\n      <td>24.5</td>\n      <td>24.0</td>\n      <td>18.5</td>\n      <td>10.0</td>\n      <td>8163.0</td>\n      <td>41.70</td>\n      <td>53.25</td>\n      <td>30.15</td>\n      <td>79.45</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>",
      "text/plain": "    COUNTY  DATE   CDSD   EMXP    PRCP   CLDD  DT00   DT32  DX32   DX70  ...  \\\n0  Addison  1990  314.0  2.520  48.940  314.0   2.0  136.0  55.0  107.0  ...   \n1  Addison  1991  314.0  1.900  37.150  314.0  30.0  161.0  63.0  109.0  ...   \n2  Addison  1992  209.0  1.660  34.510  209.0  28.0  174.0  58.0  105.0  ...   \n3  Addison  1993  264.5  1.925  39.240  264.5  42.0  192.0  70.5  105.5  ...   \n4  Addison  1994  325.5  1.520  40.745  325.5  42.0  180.5  69.0  102.0  ...   \n\n   FZF5  FZF6  FZF7  FZF8  FZF9    HTDD   TAVG   TMAX   TMIN    SNOW  \n0  24.0  24.0  24.0  17.0  11.0  7198.0  47.00  54.20  38.50   69.40  \n1  30.0  27.0  20.0  20.0  11.0  7941.5  42.90  53.60  32.30   57.70  \n2  31.0  26.0  24.0  15.0  15.0  7909.5  43.90  53.40  31.40   22.50  \n3  30.5  28.0  21.5  19.5   9.5  8435.5  41.25  53.10  29.40  129.95  \n4  30.5  24.5  24.0  18.5  10.0  8163.0  41.70  53.25  30.15   79.45  \n\n[5 rows x 28 columns]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.to_csv(\"../../data/cleaned/environment/environment clustering.csv\", index=False)\n",
    "predictions.to_csv(\"../../data/cleaned/environment/environment predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant.to_csv(\"../../data/cleaned/environment/cleaned environment data.csv\", index=False)"
   ]
  }
 ]
}
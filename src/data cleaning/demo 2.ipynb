{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37064bitfcde8f5681344d079959018a3393bc83",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_url = \"../../data/raw/environment data.csv\"\n",
    "raw_dest = \"../../data/raw/environment data - with counties.csv\"\n",
    "\n",
    "raw_grand_isle_url = \"../../data/raw/grand isle.csv\"\n",
    "raw_grand_isle_dest = \"../../data/raw/grand isle - with counties.csv\"\n",
    "\n",
    "relevant_file_url = \"../../data/raw/environment data - with counties.csv\"\n",
    "relevant_grand_isle_url = \"../../data/raw/grand isle - with counties.csv\"\n",
    "\n",
    "relevant_cleaned_dest = \"../../data/cleaned/environment/cleaned environment data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(url):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(url)\n",
    "    # Strip extra spaces from column names\n",
    "    data.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "    # Insert column \"COUNTY\" into 4th index with no values and allowing duplicates\n",
    "    data.insert(4, \"COUNTY\", None, True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def convert_coords_to_county(data, dest, write=True):\n",
    "# CONVERT LAT AND LONG --> COUNTY NAME\n",
    "    stations = data.STATION.unique()\n",
    "    start = time.time()\n",
    "    for station in stations:\n",
    "        # get lat and long values from the new data for that station\n",
    "        latitude, longitude = data[data['STATION'] == station].values[0][2:4]\n",
    "        # create json payload with corresponding lat long values\n",
    "        payload = { 'latitude': latitude, 'longitude': longitude, 'format': 'json' }\n",
    "        r = requests.get('https://geo.fcc.gov/api/census/area', params=payload).json()\n",
    "        # get county names for each lat long and fill in new_data\n",
    "        county = r['County']['name']\n",
    "        data.loc[data['STATION'] == station, 'COUNTY'] = county\n",
    "    print(\"County names generated in {} seconds.\".format(time.time() - start))\n",
    "\n",
    "    # Write to file\n",
    "    if write:\n",
    "        data.to_csv(dest, index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def format_raw_data(url, dest, write=False):\n",
    "    data = load_raw_data(url)\n",
    "    formatted_data = convert_coords_to_county(data, dest, write)\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING THE DATA FURTHER\n",
    "\n",
    "def load_relevant_data(url):\n",
    "    # load the data\n",
    "    data = pd.read_csv(url)\n",
    "    # replace cells with only spaces with NaN\n",
    "    df = data.copy().replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    # Isolate relevant columns\n",
    "    relevant_cols = [\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"COUNTY\", \"ELEVATION\", \"DATE\", \"CDSD\", \"EMXP\", \"PRCP\", \"CLDD\", \"DT00\", \"DT32\", \"DX32\", \"DX70\", \"DX90\", \"EMNT\", \"EMXT\", \"FZF0\", \"FZF1\", \"FZF2\", \"FZF3\", \"FZF4\", \"FZF5\", \"FZF6\", \"FZF7\", \"FZF8\", \"FZF9\", \"HTDD\", \"TAVG\", \"TMAX\", \"TMIN\", \"SNOW\"]\n",
    "\n",
    "    # Isolate data from relevant columns and drop duplicate rows\n",
    "    relevant = df[relevant_cols].drop_duplicates()\n",
    "    # drop columns with no null values / we don't care about for cleaning\n",
    "    relevant = relevant.sort_values(by=[\"COUNTY\", \"DATE\"]).reset_index(drop=True).drop([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"], axis=1)\n",
    "    #display(relevant.head(3))\n",
    "\n",
    "    return relevant\n",
    "\n",
    "# CLEAN UP NULL VALUES IN DATA\n",
    "\n",
    "def clean_null_by_county(data):\n",
    "    df = data.copy()\n",
    "    # group the data by county and date\n",
    "    relevant_groupby = df.groupby([\"COUNTY\", \"DATE\"])\n",
    "\n",
    "    for county in df[\"COUNTY\"].unique():\n",
    "        # slice dataframe to only have data from a single county\n",
    "        county_data = df[df[\"COUNTY\"] == county]\n",
    "        for year in county_data[\"DATE\"].unique():\n",
    "            # get current slice of data from that year for that county\n",
    "            current_group = relevant_groupby.get_group((county, year))\n",
    "            # get the names of the columns that are missing all their data\n",
    "            missing_cols = current_group.loc[:, current_group.isna().sum() == current_group.shape[0]].columns\n",
    "            # get the names of the columns that have some missing data but not all\n",
    "            other_cols = [col for col in current_group.columns if col not in missing_cols]\n",
    "            # fill in the missing values in the columns in other_cols with their respective medians\n",
    "            current_group[other_cols] = current_group[other_cols].fillna(current_group[other_cols].median())\n",
    "            # save values in current group back into original dataframe ('relevant')\n",
    "            df.update(current_group, overwrite=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# CLEAN UP REMAINING NULL VALUES\n",
    "def clean_remaining_nulls(df):\n",
    "    # SPLIT THE DATA INTO TWO - ONE FOR PREDICTIONS, ONE FOR CLUSTERING\n",
    "    clustering = df.copy()\n",
    "    clustering = clustering.fillna(-9999999) # WIP\n",
    "\n",
    "    predictions = df.copy()\n",
    "    for col in predictions.columns:\n",
    "        missing_percentage = df[col].isna().sum() / predictions.shape[0] * 100\n",
    "        # set a threshhold to drop columns\n",
    "        if missing_percentage < 50.0:\n",
    "            predictions[col] = predictions[col].interpolate(method=\"linear\")\n",
    "        # if it exceeds it, drop the column?\n",
    "        else:\n",
    "            predictions = predictions.drop(columns=[col], axis=1)\n",
    "\n",
    "    predictions = predictions.fillna(predictions.median()).groupby([\"COUNTY\", \"DATE\"]).median().reset_index()\n",
    "    \n",
    "    \n",
    "    return clustering, predictions\n",
    "\n",
    "def clean_env_data(url, dest, fill=True, write=True):\n",
    "    cleaning_time_start = time.time()\n",
    "    \n",
    "    data = load_relevant_data(url)\n",
    "    cleaned_data = clean_null_by_county(data)\n",
    "    \n",
    "    if fill:\n",
    "        cleaned_clustering, cleaned_predictions = clean_remaining_nulls(cleaned_data)\n",
    "        if write:\n",
    "            cleaned_clustering.to_csv(dest.replace(\".csv\", \" - clustering.csv\"), index=False)\n",
    "            cleaned_predictions.to_csv(dest.replace(\".csv\", \" - clustering.csv\"), index=False)\n",
    "        print(\"Data cleaned in {} seconds.\".format(time.time() - cleaning_time_start))\n",
    "        return cleaned_clustering, cleaned_predictions\n",
    "    else:\n",
    "        if write:\n",
    "            cleaned_data.to_csv(dest, index=False)\n",
    "        print(\"Data cleaned in {} seconds.\".format(time.time() - cleaning_time_start))\n",
    "    \n",
    "        return cleaned_data\n",
    "\n",
    "def completely_clean(raw_url, raw_dest, relevant_url, relevant_dest, write_raw=False, fill_all_nulls=True, write_relevant=True):\n",
    "    formatted_data = format_raw_data(raw_url, raw_dest, write_raw)\n",
    "    fully_cleaned_data = clean_env_data(relevant_url, relevant_dest, fill, write_relevant)\n",
    "\n",
    "    return formatted_data, fully_cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "County names generated in 26.778244733810425 seconds.\nCounty names generated in 0.16954851150512695 seconds.\n"
    }
   ],
   "source": [
    "## Testing raw data generation\n",
    "raw_environment_data = format_raw_data(raw_file_url, raw_dest, True)\n",
    "raw_grand_isle = format_raw_data(raw_grand_isle_url, raw_grand_isle_dest, True)\n",
    "\n",
    "# Run once\n",
    "# raw_combined = pd.concat([raw_environment_data, raw_grand_isle])\n",
    "# raw_combined.to_csv(raw_dest, index=False)\n",
    "\n",
    "# Testing loading relevant data (raw)\n",
    "# formatted_grand_isle = load_relevant_data(relevant_grand_isle_url)\n",
    "# formatted_all = load_relevant_data(relevant_file_url)\n",
    "# merged = pd.concat([formatted_grand_isle, formatted_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data cleaned in 11.160754203796387 seconds.\n"
    }
   ],
   "source": [
    "clustering, predictions = clean_env_data(relevant_file_url, relevant_cleaned_dest, True, True)"
   ]
  }
 ]
}